import json
from pathlib import Path
from typing import cast

import numpy as np
import timm
import torch
from PIL import Image
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms
from tqdm import tqdm
from .augmentations import build_img_transforms


class ImageDatasetForEmbeddings(Dataset):
    def __init__(self, image_dir: Path, captions_path: Path):
        self.image_dir = image_dir

        # –ó–∞–≥—Ä—É–∂–∞–µ–º captions —á—Ç–æ–±—ã –ø–æ–Ω—è—Ç—å –∫–∞–∫–∏–µ –º–æ–¥–µ–ª–∏ –∏–º–µ—é—Ç —Ç–µ–∫—Å—Ç
        with open(captions_path, 'r') as f:
            captions = json.load(f)

        # IDs –º–æ–¥–µ–ª–µ–π —Å —Ç–µ–∫—Å—Ç–æ–º (0-524)
        text_model_ids = set(captions.keys())

        # –ù–µ–≤–∞–ª–∏–¥–Ω—ã–µ IDs –∫–æ—Ç–æ—Ä—ã–µ –Ω–∞–¥–æ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å
        self.invalid_ids = {'0654', '2764', '3818', '4263'}

        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        all_images = sorted(self.image_dir.glob("*.png"))

        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –º–æ–¥–µ–ª–∏ 525+ (–±–µ–∑ —Ç–µ–∫—Å—Ç–∞) –∏ –Ω–µ –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–µ
        self.image_paths = []
        for img_path in all_images:
            model_id = img_path.stem.split('_')[0]

            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–µ
            if model_id in self.invalid_ids:
                continue

            # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –º–æ–¥–µ–ª–∏ –±–µ–∑ —Ç–µ–∫—Å—Ç–∞ (525+)
            if model_id not in text_model_ids:
                self.image_paths.append(img_path)

        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(self.image_paths)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤")
        print(f"   (–º–æ–¥–µ–ª–∏ 525+, –∏—Å–∫–ª—é—á–∞—è –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–µ)")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —É –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π –ø–æ 24 views
        model_views = {}
        for path in self.image_paths:
            model_id = path.stem.split('_')[0]
            if model_id not in model_views:
                model_views[model_id] = []
            view_idx = int(path.stem.split('_')[1])
            model_views[model_id].append(view_idx)

        # –ü—Ä–æ–≤–µ—Ä–∫–∞
        for model_id, views in model_views.items():
            if len(views) != 24:
                raise Exception(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å {model_id} –∏–º–µ–µ—Ç {len(views)} views –≤–º–µ—Å—Ç–æ 24!")

        self.transform = build_img_transforms(336)

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        img_path = self.image_paths[idx]
        img = Image.open(img_path).convert("RGB")
        img_tensor = self.transform(img)
        return img_tensor, img_path.name


def extract_and_save_embeddings(
        train_images_dir: Path,
        captions_path: Path,
        output_path: Path,
        model_name: str = "eva_large_patch14_336.in22k_ft_in22k_in1k",
        batch_size: int = 64,
        device: str = "cuda",
):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –≤—Å–µ—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –º–æ–¥–µ–ª–µ–π 525+ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ npz"""

    # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
    dataset = ImageDatasetForEmbeddings(train_images_dir, captions_path)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0)

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
    print(f"\nüì¶ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {model_name}...")
    model = timm.create_model(model_name, pretrained=True, num_classes=0)
    model = model.to(device)
    model.eval()

    # –ò–∑–≤–ª–µ–∫–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
    all_embeddings = []
    all_filenames = []

    print("\nüîÑ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
    with torch.no_grad():
        for batch_imgs, batch_names in tqdm(dataloader, desc="Extracting embeddings"):
            batch_imgs = batch_imgs.to(device)
            embeddings = model(batch_imgs)

            all_embeddings.append(embeddings.cpu().numpy())
            all_filenames.extend(batch_names)

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
    embeddings_array = np.vstack(all_embeddings)
    filenames_array = np.array(all_filenames)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ {output_path}...")
    np.savez_compressed(
        output_path,
        embeddings=embeddings_array,
        filenames=filenames_array
    )

    print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(embeddings_array)} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤")
    print(f"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {embeddings_array.shape}")

    return embeddings_array, filenames_array