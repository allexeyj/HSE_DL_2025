import torch
import torch.nn as nn
import torch.nn.functional as F
from easydict import EasyDict as edict
import open_clip

from ReConV2.models.ReCon import ReCon2

from .losses import ContrastiveLoss


class BasePcEncoder(nn.Module):
    def __init__(self, config: edict):
        super().__init__()
        self.text_ratio = config.text_ratio
        self.pc_encoder_base = ReCon2(config.model)
        self.config = config

    def encode_pc(self, pc: torch.Tensor, normalize: bool) -> torch.Tensor:
        """ÐšÐ¾Ð´Ð¸Ñ€ÑƒÐµÑ‚ point cloud, ÑÐ¾Ð²Ð¼ÐµÑ‰Ð°Ñ Ñ‚Ð¾ÐºÐµÐ½Ñ‹ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹ Ð¸ Ñ‚ÐµÐºÑÑ‚Ð°."""
        img_token, text_token, _, _ = self.pc_encoder_base.forward_features(pc)
        img_pred_feat = torch.mean(img_token, dim=1)
        text_pred_feat = torch.mean(text_token, dim=1)
        pc_feats = img_pred_feat + text_pred_feat * self.text_ratio
        return F.normalize(pc_feats, dim=-1) if normalize else pc_feats


class TextMeshRetrievalModel(nn.Module):
    """ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð´Ð»Ñ text-mesh retrieval"""

    def __init__(self, config: edict, recon_ckpt: str) -> None:
        super().__init__()

        # PC encoder
        self.pc_encoder = BasePcEncoder(config)

        # Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ Ð²ÐµÑÐ° ReConV2 (Ð¾Ð±ÑÐ·Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾)
        ckpt = torch.load(recon_ckpt, map_location="cpu")
        self.pc_encoder.pc_encoder_base.load_state_dict(ckpt, strict=True)
        print("âœ… ReCon++ checkpoint loaded successfully")

        # Ð—Ð°Ð¼Ð¾Ñ€Ð°Ð¶Ð¸Ð²Ð°ÐµÐ¼ PC encoder Ð¿Ð¾Ð»Ð½Ð¾ÑÑ‚ÑŒÑŽ
        for param in self.pc_encoder.parameters():
            param.requires_grad = False

        frozen_params = sum(p.numel() for p in self.pc_encoder.parameters())
        print(f"ðŸ”’ PC Encoder Ð¿Ð¾Ð»Ð½Ð¾ÑÑ‚ÑŒÑŽ Ð·Ð°Ð¼Ð¾Ñ€Ð¾Ð¶ÐµÐ½: {frozen_params:,} params (~{frozen_params / 1e6:.2f}M)")

        # Text encoder Ñ Ñ€Ð°Ð·Ð¼Ð¾Ñ€Ð¾Ð¶ÐµÐ½Ð½Ñ‹Ð¼Ð¸ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ð¼Ð¸ 4 ÑÐ»Ð¾ÑÐ¼Ð¸
        self.text_encoder = TextEncoder(config)

        # Loss
        self.contrastive_loss = ContrastiveLoss(
            init_temp=config.train_params.temperature
        )

    def forward(self, pc_batch: torch.Tensor, texts: list[str]) -> torch.Tensor:
        """Forward pass Ð´Ð»Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ"""
        # ÐšÐ¾Ð´Ð¸Ñ€ÑƒÐµÐ¼ PC (Ð±ÐµÐ· Ð³Ñ€Ð°Ð´Ð¸ÐµÐ½Ñ‚Ð¾Ð², Ñ‚.Ðº. Ð·Ð°Ð¼Ð¾Ñ€Ð¾Ð¶ÐµÐ½)
        with torch.no_grad():
            pc_embeddings = self.pc_encoder.encode_pc(pc_batch, normalize=True)

        # ÐšÐ¾Ð´Ð¸Ñ€ÑƒÐµÐ¼ Ñ‚ÐµÐºÑÑ‚Ñ‹ (Ñ Ð³Ñ€Ð°Ð´Ð¸ÐµÐ½Ñ‚Ð°Ð¼Ð¸)
        text_embeddings = self.text_encoder.encode_text(texts, normalize=True)

        # Ð’Ñ‹Ñ‡Ð¸ÑÐ»ÑÐµÐ¼ loss
        loss = self.contrastive_loss(pc_embeddings.detach(), text_embeddings)
        return loss


class TextEncoder(nn.Module):
    """Text encoder Ð½Ð° Ð±Ð°Ð·Ðµ open_clip EVA02-L-14-336 Ñ Ñ€Ð°Ð·Ð¼Ð¾Ñ€Ð¾Ð·ÐºÐ¾Ð¹ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ñ… 4 ÑÐ»Ð¾ÐµÐ²"""

    def __init__(self, config: edict) -> None:
        super().__init__()
        self.config = config

        # Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ EVA Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð¸ Ñ‚Ð¾ÐºÐµÐ½Ð¸Ð·Ð°Ñ‚Ð¾Ñ€
        model, _, _ = open_clip.create_model_and_transforms(
            'EVA02-L-14-336',
            pretrained='merged2b_s6b_b61k'
        )
        self.text_encoder = model
        self.tokenizer = open_clip.get_tokenizer('EVA02-L-14-336')

        # Ð—Ð°Ð¼Ð¾Ñ€Ð°Ð¶Ð¸Ð²Ð°ÐµÐ¼ Ð²ÑÐµ Ð²ÐµÑÐ° Ð¸Ð·Ð½Ð°Ñ‡Ð°Ð»ÑŒÐ½Ð¾
        for param in self.text_encoder.parameters():
            param.requires_grad = False

        # Ð Ð°Ð·Ð¼Ð¾Ñ€Ð°Ð¶Ð¸Ð²Ð°ÐµÐ¼ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ðµ 4 ÑÐ»Ð¾Ñ
        resblocks = self.text_encoder.text.transformer.resblocks
        total_blocks = len(resblocks)

        # Ð Ð°Ð·Ð¼Ð¾Ñ€Ð°Ð¶Ð¸Ð²Ð°ÐµÐ¼ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ðµ 4 Ð±Ð»Ð¾ÐºÐ°
        for block in resblocks[-4:]:
            for param in block.parameters():
                param.requires_grad = True

        # Ð¢Ð°ÐºÐ¶Ðµ Ñ€Ð°Ð·Ð¼Ð¾Ñ€Ð°Ð¶Ð¸Ð²Ð°ÐµÐ¼ Ñ„Ð¸Ð½Ð°Ð»ÑŒÐ½Ñ‹Ð¹ LayerNorm
        for param in self.text_encoder.text.ln_final.parameters():
            param.requires_grad = True

        trainable_params = sum(p.numel() for p in self.text_encoder.parameters() if p.requires_grad)
        print(f"âœ… Text encoder EVA02-L-14-336 loaded")
        print(f"   ðŸ”“ Ð Ð°Ð·Ð¼Ð¾Ñ€Ð¾Ð¶ÐµÐ½Ñ‹ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ðµ 4 ÑÐ»Ð¾Ñ text transformer")
        print(f"   ðŸ“Š Trainable params in text encoder: {trainable_params:,} (~{trainable_params / 1e6:.2f}M)")

        # ÐŸÑ€Ð¾ÐµÐºÑ†Ð¸Ð¾Ð½Ð½Ð°Ñ Ð³Ð¾Ð»Ð¾Ð²Ð° (Ð²ÑÐµÐ³Ð´Ð° Ð¾Ð±ÑƒÑ‡Ð°ÐµÐ¼Ð°Ñ)
        text_dim = 768  # EVA02-L-14-336 text embedding dim
        self.text_proj = nn.Sequential(
            nn.Linear(text_dim, config.emb_dim),
            nn.ReLU(),
            nn.Linear(config.emb_dim, config.emb_dim)
        )

    def encode_text(self, texts: list[str], normalize: bool = True) -> torch.Tensor:
        """ÐšÐ¾Ð´Ð¸Ñ€ÑƒÐµÑ‚ ÑÐ¿Ð¸ÑÐ¾Ðº Ñ‚ÐµÐºÑÑ‚Ð¾Ð²"""
        # Ð¢Ð¾ÐºÐµÐ½Ð¸Ð·Ð°Ñ†Ð¸Ñ
        tokens = self.tokenizer(texts).to(self.config.device)

        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¸
        text_features = self.text_encoder.encode_text(tokens)

        # ÐŸÑ€Ð¾ÐµÑ†Ð¸Ñ€ÑƒÐµÐ¼ Ñ‡ÐµÑ€ÐµÐ· Ð¾Ð±ÑƒÑ‡Ð°ÐµÐ¼ÑƒÑŽ Ð³Ð¾Ð»Ð¾Ð²Ñƒ
        text_embeddings = self.text_proj(text_features.float())

        return F.normalize(text_embeddings, dim=-1) if normalize else text_embeddings
class InferencePcEncoder(BasePcEncoder):
    """PC encoder Ð´Ð»Ñ Ð¸Ð½Ñ„ÐµÑ€ÐµÐ½ÑÐ°"""

    def __init__(self, config: edict) -> None:
        super().__init__(config)

    def load_pc_encoder_weights(self, checkpoint_path: str) -> None:
        checkpoint = torch.load(checkpoint_path, map_location="cpu")
        self.pc_encoder_base.load_state_dict(checkpoint, strict=True)
        print(f"âœ… PC encoder weights loaded from {checkpoint_path}")


class InferenceTextEncoder(nn.Module):
    """Text encoder Ð´Ð»Ñ Ð¸Ð½Ñ„ÐµÑ€ÐµÐ½ÑÐ°"""

    def __init__(self, config: edict) -> None:
        super().__init__()
        # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð¿Ð¾Ð»Ð½Ñ‹Ð¹ TextEncoder
        self.encoder = TextEncoder(config)

    def load_text_weights(self, text_proj_path: str, text_encoder_path: str | None = None) -> None:
        """Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÑ‚ Ð²ÐµÑÐ° text projection Ð¸ (Ð¾Ð¿Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾) text encoder"""
        # Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ text projection (Ð²ÑÐµÐ³Ð´Ð°)
        checkpoint = torch.load(text_proj_path, map_location="cpu")
        self.encoder.text_proj.load_state_dict(checkpoint, strict=True)
        print(f"âœ… Text projection weights loaded from {text_proj_path}")

        # Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ Ð²ÐµÑÐ° text encoder ÐµÑÐ»Ð¸ Ð±Ñ‹Ð»Ð¸ Ñ€Ð°Ð·Ð¼Ð¾Ñ€Ð¾Ð¶ÐµÐ½Ñ‹ ÑÐ»Ð¾Ð¸
        if text_encoder_path is not None:
            checkpoint = torch.load(text_encoder_path, map_location="cpu")
            # Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ñ‚Ðµ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ ÐµÑÑ‚ÑŒ Ð² Ñ‡ÐµÐºÐ¿Ð¾Ð¸Ð½Ñ‚Ðµ
            missing, unexpected = self.encoder.text_encoder.load_state_dict(checkpoint, strict=False)
            print(f"âœ… Text encoder weights loaded from {text_encoder_path}")
            if missing:
                print(f"   â„¹ï¸  Missing keys (expected, frozen params): {len(missing)}")
            if unexpected:
                print(f"   âš ï¸  Unexpected keys: {unexpected}")

    def encode_text(self, texts: list[str], normalize: bool = True) -> torch.Tensor:
        return self.encoder.encode_text(texts, normalize)